{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier as LC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier as AB\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ET\n",
    "from os.path import dirname, abspath, join\n",
    "import praw\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from  sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "from sklearn.neural_network import MLPRegressor as MLPR\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = dirname(abspath(''))\n",
    "train_dic = join(d, 'dataset/Phase 2 - stockDirection/train.csv')\n",
    "test_dic = join(d, 'dataset/Phase 2 - stockDirection/test.csv')\n",
    "c_dic = join(d, 'dataset/Phase 2 - stockDirection/combined_stock_data.csv')\n",
    "sub_dic = join(d,  'submission/Phase 2 - submission/submission_phase2_yourteamname.csv' )\n",
    "write_dic = join(d, 'submission/Phase 2 - submission/submission_phase2_team1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all data\n",
    "raw_training = pd.DataFrame.from_csv(train_dic)\n",
    "raw_test = pd.DataFrame.from_csv(test_dic)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "cdata = pd.DataFrame.from_csv(c_dic)\n",
    "sub_data = pd.DataFrame.from_csv(sub_dic, index_col=None)\n",
    "sub_data.reset_index(inplace = True)\n",
    "cdata['Date']= [datetime.datetime.strptime(cdata['Date'][i], \"%Y-%m-%d\")for i in range(len(cdata))]\n",
    "raw_training.sort_index(inplace = True)\n",
    "raw_training.reset_index(inplace = True)\n",
    "raw_test.reset_index(inplace = True)\n",
    "raw_test.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5380263984915148"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_training['Close'] - raw_training['Open'] > 0).count(True)/len(raw_training['Close'] - raw_training['Open'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_acc(test):\n",
    "    p_i, cor, icor = 0, 0, 0\n",
    "    for c_i in range(len(cdata)):\n",
    "        if p_i == len(test):\n",
    "            break\n",
    "        if cdata['Date'][c_i] == test['Date'][p_i]:\n",
    "            if cdata['Label'][c_i] == test['prediction'][p_i]:\n",
    "                cor += 1\n",
    "            else:\n",
    "                icor += 1\n",
    "            p_i += 1\n",
    "    print(cor/(cor+icor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_data(sub_data, test):\n",
    "    for i in range(len(sub_data)):\n",
    "        d = datetime.datetime.strptime(sub_data['Date'][i], \"%m/%d/%y\")\n",
    "        sub_data['stock_directionality'][i]  = int(test['prediction'][test['Date'] == d].tolist()[0])\n",
    "\n",
    "\n",
    "    sub_data.drop('index', axis= 1, inplace = True)\n",
    "    sub_data.to_csv(write_dic, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#give predictrion\n",
    "tr_next,tr_before, te_i = 1, 0, 0\n",
    "before_date, before_close = raw_training['Date'][tr_before], raw_training['Close'][tr_before]\n",
    "nxt_date, nxt_open = raw_training['Date'][tr_next], raw_training['Open'][tr_next]\n",
    "prediction = []\n",
    "while te_i < len(raw_test):\n",
    "    te_date = raw_test['Date'][te_i]\n",
    "    while nxt_date < te_date:\n",
    "        tr_next += 1\n",
    "        nxt_date, nxt_open = raw_training['Date'][tr_next], raw_training['Open'][tr_next]\n",
    "    while raw_training['Date'][tr_before+1] < te_date:\n",
    "        tr_before += 1\n",
    "        before_date, before_close = raw_training['Date'][tr_before], raw_training['Close'][tr_before]\n",
    "    if nxt_open > before_close:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)\n",
    "    te_i += 1\n",
    "raw_test['prediction'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832146490335707\n",
      "818 165\n"
     ]
    }
   ],
   "source": [
    "cor, incor = 0, 0\n",
    "for i in range(1, len(raw_training)-1):\n",
    "    dif = max(raw_training['Date'][i+1] - raw_training['Date'][i], raw_training['Date'][i] - raw_training['Date'][i-1])\n",
    "    if dif > datetime.timedelta(days = 1):\n",
    "        pre_close, after_open, real = raw_training['Close'][i-1], raw_training['Open'][i+1], raw_training['Close'][i] - raw_training['Open'][i]\n",
    "        if (after_open - pre_close )* real > 0:\n",
    "            cor += 1\n",
    "        else:\n",
    "            incor += 1\n",
    "print(cor/(cor+incor))\n",
    "print(cor, incor)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(model, test_x, test_y):\n",
    "    pre = model.predict(test_x)\n",
    "    res = [pre[i]== test_y[i] for i in range(len(test_y))]\n",
    "    return 1.0* res.count(True)/len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1591.000000\n",
       "mean       97.666400\n",
       "std        98.530411\n",
       "min         0.000000\n",
       "25%        27.880375\n",
       "50%        68.000000\n",
       "75%       134.524905\n",
       "max       886.399902\n",
       "dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = abs(raw_training['Close'] - raw_training['Open'])\n",
    "dis.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_words(train_index, raw_training):\n",
    "    word_dic = {}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    regex = re.compile('[^a-z]') \n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for i in train_index:\n",
    "        for j in range(1, 26):\n",
    "            if type(raw_training['Top'+str(j)][i]) != str:\n",
    "                continue\n",
    "\n",
    "            news = raw_training['Top'+str(j)][i][2:-1]\n",
    "            news = news.lower()\n",
    "\n",
    "            news = regex.sub(' ', news) \n",
    "            news = ' '.join(news.split())\n",
    "            news = news.replace('u s', 'us')\n",
    "            news = news.replace('u k', 'uk')\n",
    "            news = news.replace('u n', 'un')\n",
    "            news = news.replace('n korea', 'north korea')\n",
    "            news = news.replace('e mail', 'email')\n",
    "            for w in news.split():\n",
    "\n",
    "                if w in stop_words or ps.stem(w) in stop_words:\n",
    "                    continue\n",
    "                    \n",
    "                word_dic[ps.stem(w)] = word_dic.get(ps.stem(w), 0) + 1\n",
    "    \n",
    "    keywords = {key:index for index, key in enumerate(word_dic.keys()) if word_dic[key] > 50}\n",
    "    influence = {}\n",
    "    for i in train_index:\n",
    "        for j in range(1, 26):\n",
    "            if type(raw_training['Top'+str(j)][i]) != str:\n",
    "                continue\n",
    "\n",
    "            news = raw_training['Top'+str(j)][i][2:-1]\n",
    "            news = news.lower()\n",
    "\n",
    "            news = regex.sub(' ', news) \n",
    "            news = ' '.join(news.split())\n",
    "            news = news.replace('u s', 'us')\n",
    "            news = news.replace('u k', 'uk')\n",
    "            news = news.replace('u n', 'un')\n",
    "            news = news.replace('n korea', 'north korea')\n",
    "            news = news.replace('e mail', 'email')\n",
    "            for w in news.split():\n",
    "                stem = ps.stem(w)\n",
    "                if stem in keywords:\n",
    "                    influence[stem] = influence.get(stem, 0) +  raw_training['Close'][i] - raw_training['Open'][i]\n",
    "    avg_dif = {}\n",
    "    for key in influence:\n",
    "        avg_dif[key] = abs(influence[key]/word_dic[key])\n",
    "    return avg_dif\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_set(df, sid, train_set, valid_set, last_days):\n",
    "    train_x, valid_x, train_y, valid_y  = [], [], [], []\n",
    "    sentiment_keys = ['compound', 'neg', 'pos']\n",
    "    \"\"\"\n",
    "    avg_dif_dic = imp_words(train_set, raw_training)\n",
    "    i = 0\n",
    "    imp_words_dic = {}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    regex = re.compile('[^a-z]') \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    for key in avg_dif_dic:\n",
    "        #avg influence at least be 50\n",
    "        if avg_dif_dic[key]>30:\n",
    "            imp_words_dic[key] =  i\n",
    "            i += 1\n",
    "    \"\"\"\n",
    "    \n",
    "    for row_index in range(len(df)):\n",
    "        increase = None\n",
    "        # ylabel\n",
    "        if df['Close'][row_index] - df['Open'][row_index] >= 0:\n",
    "            increase = 1\n",
    "        else:\n",
    "            increase = 0\n",
    "        d = []\n",
    "        #imp_list= [0]*len(imp_words_dic.keys())\n",
    "        for j in range(1, 26):\n",
    "            #sentiment score\n",
    "            polarity_scores = {'neu':0, 'pos':0, 'neg':0,'compound':0}\n",
    "            try:\n",
    "                polarity_scores = sid.polarity_scores(df['Top' + str(j)][row_index])\n",
    "            except:\n",
    "                pass\n",
    "            for key in sentiment_keys:\n",
    "                d.append(polarity_scores[key])\n",
    "            #important words\n",
    "            \"\"\"\n",
    "            if type(df['Top'+str(j)][row_index]) != str:\n",
    "                continue\n",
    "\n",
    "            news = raw_training['Top'+str(j)][row_index][2:-1]\n",
    "            news = news.lower()\n",
    "\n",
    "            news = regex.sub(' ', news) \n",
    "            news = ' '.join(news.split())\n",
    "            news = news.replace('u s', 'us')\n",
    "            news = news.replace('u k', 'uk')\n",
    "            news = news.replace('u n', 'un')\n",
    "            news = news.replace('n korea', 'north korea')\n",
    "            news = news.replace('e mail', 'email')\n",
    "            for w in news.split():\n",
    "                stem = ps.stem(w)\n",
    "                if stem in imp_words_dic:\n",
    "                    imp_list[imp_words_dic[stem]] += 1\n",
    "            \n",
    "        d += imp_list\n",
    "        \"\"\"\n",
    "        last = pd.Series([None] * last_days)\n",
    "        # last n days data\n",
    "        try:\n",
    "            for i in range(last_days):\n",
    "                last[i] = raw_training['Close'][row_index-1-i] - raw_training['Open'][row_index-1-i]\n",
    "        except:\n",
    "            pass\n",
    "        d += list(last)\n",
    "        \n",
    "        if row_index in train_set:\n",
    "            train_x.append(d)\n",
    "            train_y.append(increase)\n",
    "        else:\n",
    "            valid_x.append(d)\n",
    "            valid_y.append(increase)\n",
    "        \n",
    "    column_names = []\n",
    "    for i in range(1, 26):\n",
    "        for key in sentiment_keys:\n",
    "            column_names.append('Top'+str(i)+'_'+key)\n",
    "    #column_names += list(imp_words_dic.keys())\n",
    "    column_names += ['last'+str(i+1) for i in range(last_days)]\n",
    "    train_df = pd.DataFrame(data = train_x, columns = column_names)\n",
    "    valid_df = pd.DataFrame(data = valid_x, columns = column_names)\n",
    "    return train_df, train_y, valid_df, valid_y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_set(df, sid, last_days):\n",
    "    test_x  = []\n",
    "    sentiment_keys = ['compound', 'neg', 'pos']\n",
    "    for row_index in range(len(df)): \n",
    "        d = []\n",
    "        for j in range(1, 26):\n",
    "            #sentiment score\n",
    "            polarity_scores = {'neu':0, 'pos':0, 'neg':0,'compound':0}\n",
    "            try:\n",
    "                polarity_scores = sid.polarity_scores(df['Top' + str(j)][row_index])\n",
    "            except:\n",
    "                pass\n",
    "            for key in sentiment_keys:\n",
    "                d.append(polarity_scores[key])\n",
    "        last = pd.Series([None] * last_days)\n",
    "        d += list(last)\n",
    "        test_x.append(d)\n",
    "    column_names = []\n",
    "    for i in range(1, 26):\n",
    "        for key in sentiment_keys:\n",
    "            column_names.append('Top'+str(i)+'_'+key)\n",
    "    column_names += ['last'+str(i+1) for i in range(last_days)]\n",
    "    test_df = pd.DataFrame(data = test_x, columns = column_names)\n",
    "    return test_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172413793103449"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "index_list= [i for i in range(len(raw_training))]\n",
    "random.shuffle(index_list)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "last_days = 0\n",
    "train_set, test_set = set(index_list[:int(train_ratio*len(raw_training))]), set(index_list[int(train_ratio*len(raw_training)):])\n",
    "train_x, train_y, valid_x, valid_y = generate_training_set(raw_training, sid, train_set, test_set, last_days)\n",
    "for n in train_x.columns:\n",
    "    train_x[n].fillna(train_x[n].mean(), inplace = True)\n",
    "    valid_x[n].fillna(train_x[n].mean(), inplace = True)\n",
    "sum([1 for i in range(len(valid_y)) if valid_y[i] > 0 ])/len(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top1_compound</th>\n",
       "      <th>Top2_compound</th>\n",
       "      <th>Top3_compound</th>\n",
       "      <th>Top4_compound</th>\n",
       "      <th>Top5_compound</th>\n",
       "      <th>Top6_compound</th>\n",
       "      <th>Top7_compound</th>\n",
       "      <th>Top8_compound</th>\n",
       "      <th>Top9_compound</th>\n",
       "      <th>Top10_compound</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16_compound</th>\n",
       "      <th>Top17_compound</th>\n",
       "      <th>Top18_compound</th>\n",
       "      <th>Top19_compound</th>\n",
       "      <th>Top20_compound</th>\n",
       "      <th>Top21_compound</th>\n",
       "      <th>Top22_compound</th>\n",
       "      <th>Top23_compound</th>\n",
       "      <th>Top24_compound</th>\n",
       "      <th>Top25_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>-0.7089</td>\n",
       "      <td>-0.9260</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2732</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.7579</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>-0.2755</td>\n",
       "      <td>-0.8519</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8156</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>-0.1965</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>-0.6688</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.8020</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>-0.1832</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.7184</td>\n",
       "      <td>-0.8074</td>\n",
       "      <td>-0.6369</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>-0.8689</td>\n",
       "      <td>-0.6124</td>\n",
       "      <td>-0.6369</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>-0.6808</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>0.7096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.2732</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2732</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Top1_compound  Top2_compound  Top3_compound  Top4_compound  Top5_compound  \\\n",
       "0        -0.5994         0.0000        -0.3612        -0.7089        -0.9260   \n",
       "1         0.8156        -0.3182         0.4404        -0.1965         0.0000   \n",
       "2        -0.7184        -0.8074        -0.6369        -0.1280        -0.5106   \n",
       "3         0.2023         0.0000         0.6808        -0.8689        -0.6124   \n",
       "4        -0.2732        -0.4767        -0.8316         0.0000         0.0000   \n",
       "\n",
       "   Top6_compound  Top7_compound  Top8_compound  Top9_compound  Top10_compound  \\\n",
       "0         0.0000        -0.2732         0.2144        -0.5719         -0.5994   \n",
       "1        -0.4939        -0.5106        -0.0772        -0.6688         -0.3400   \n",
       "2         0.0000         0.2960         0.0000         0.3612          0.0000   \n",
       "3        -0.6369         0.7177        -0.4404        -0.6808         -0.3400   \n",
       "4         0.4588         0.0000        -0.1027         0.0000         -0.2732   \n",
       "\n",
       "        ...        Top16_compound  Top17_compound  Top18_compound  \\\n",
       "0       ...                0.0000          0.0000          0.2086   \n",
       "1       ...                0.0000          0.0000         -0.6597   \n",
       "2       ...                0.0000          0.0000          0.0000   \n",
       "3       ...               -0.6249          0.0000          0.0000   \n",
       "4       ...                0.0000         -0.2732          0.0000   \n",
       "\n",
       "   Top19_compound  Top20_compound  Top21_compound  Top22_compound  \\\n",
       "0          0.2023          0.0258         -0.7579         -0.6249   \n",
       "1          0.0000          0.0000         -0.8020          0.0000   \n",
       "2         -0.5423         -0.0258         -0.2960          0.4939   \n",
       "3          0.0000          0.0000         -0.4404         -0.5994   \n",
       "4          0.0000         -0.5994         -0.2960         -0.2500   \n",
       "\n",
       "   Top23_compound  Top24_compound  Top25_compound  \n",
       "0         -0.2755         -0.8519          0.4019  \n",
       "1         -0.3182         -0.1832          0.0000  \n",
       "2         -0.5719         -0.4215         -0.3400  \n",
       "3          0.1779         -0.6908          0.7096  \n",
       "4         -0.3612          0.2500          0.0000  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5235109717868338\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPC(activation = 'relu', solver = 'adam', alpha = 0.1, hidden_layer_sizes = tuple((200,)*3), tol = 0.0001,max_iter= 200000, verbose =False, learning_rate = 'adaptive' )\n",
    "mlp.fit(train_x, train_y)\n",
    "print(accuracy(mlp, valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always say yes 0.5410658307210031\n",
      "0.05\n",
      "0.4915360501567398\n",
      "0.1\n",
      "0.509090909090909\n",
      "0.5\n",
      "0.5285266457680251\n",
      "1\n",
      "0.5335423197492164\n",
      "1.5\n",
      "0.5316614420062696\n",
      "2\n",
      "0.5410658307210031\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.05, 0.1, 0.5, 1,1.5, 2]\n",
    "res = {}\n",
    "yes_res = []\n",
    "for a in alphas:\n",
    "    res[a] = []\n",
    "for i in range(5):\n",
    "    random.shuffle(index_list)\n",
    "    train_set, test_set = set(index_list[:int(train_ratio*len(raw_training))]), set(index_list[int(train_ratio*len(raw_training)):])\n",
    "    train_x, train_y, valid_x, valid_y = generate_training_set(raw_training, sid, train_set, test_set, last_days)\n",
    "    yes_res.append(sum([1 for i in range(len(valid_y)) if valid_y[i] > 0 ])/len(valid_y))\n",
    "    for a in alphas:\n",
    "        mlp = MLPC(activation = 'relu', solver = 'adam', alpha = a, hidden_layer_sizes = tuple((200,)*3), tol = 0.0001,max_iter= 200000, verbose =False, learning_rate = 'adaptive' )\n",
    "        mlp.fit(train_x, train_y)\n",
    "        res[a].append(accuracy(mlp, valid_x, valid_y))\n",
    "print('always say yes', np.average(yes_res))\n",
    "for a in alphas:\n",
    "    print(a)\n",
    "    print(np.average(res[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always say yes 0.5391849529780564\n",
      "100\n",
      "0.4921630094043887\n",
      "200\n",
      "0.5228840125391849\n",
      "300\n",
      "0.5153605015673981\n",
      "500\n",
      "0.5153605015673981\n",
      "1000\n",
      "0.5191222570532915\n"
     ]
    }
   ],
   "source": [
    "neurons = [100, 200, 300, 500, 1000]\n",
    "res = {}\n",
    "yes_res = []\n",
    "for n in neurons:\n",
    "    res[n] = []\n",
    "for i in range(5):\n",
    "    random.shuffle(index_list)\n",
    "    train_set, test_set = set(index_list[:int(train_ratio*len(raw_training))]), set(index_list[int(train_ratio*len(raw_training)):])\n",
    "    train_x, train_y, valid_x, valid_y = generate_training_set(raw_training, sid, train_set, test_set, last_days)\n",
    "    yes_res.append(sum([1 for i in range(len(valid_y)) if valid_y[i] > 0 ])/len(valid_y))\n",
    "    for n in neurons:\n",
    "        mlp = MLPC(activation = 'relu', solver = 'adam', alpha = 0.1, hidden_layer_sizes = tuple((n,)*3), tol = 0.0001,max_iter= 200000, verbose =False, learning_rate = 'adaptive' )\n",
    "        mlp.fit(train_x, train_y)\n",
    "        res[n].append(accuracy(mlp, valid_x, valid_y))\n",
    "print('always say yes', np.average(yes_res))\n",
    "for n in neurons:\n",
    "    print(n)\n",
    "    print(np.average(res[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always say yes 0.5398119122257053\n",
      "1\n",
      "0.503448275862069\n",
      "2\n",
      "0.5059561128526646\n",
      "3\n",
      "0.5047021943573669\n",
      "4\n",
      "0.5216300940438872\n",
      "5\n",
      "0.5398119122257053\n"
     ]
    }
   ],
   "source": [
    "layers = [1,2,3,4,5]\n",
    "res = {}\n",
    "yes_res = []\n",
    "for l in layers:\n",
    "    res[l] = []\n",
    "for i in range(5):\n",
    "    random.shuffle(index_list)\n",
    "    train_set, test_set = set(index_list[:int(train_ratio*len(raw_training))]), set(index_list[int(train_ratio*len(raw_training)):])\n",
    "    train_x, train_y, valid_x, valid_y = generate_training_set(raw_training, sid, train_set, test_set, last_days)\n",
    "    yes_res.append(sum([1 for i in range(len(valid_y)) if valid_y[i] > 0 ])/len(valid_y))\n",
    "    for l in layers:\n",
    "        mlp = MLPC(activation = 'relu', solver = 'adam', alpha = 1, hidden_layer_sizes = tuple((200,)*l), tol = 0.0001,max_iter= 200000, verbose =False, learning_rate = 'adaptive' )\n",
    "        mlp.fit(train_x, train_y)\n",
    "        res[l].append(accuracy(mlp, valid_x, valid_y))\n",
    "print('always say yes', np.average(yes_res))\n",
    "for l in layers:    \n",
    "    print(l)\n",
    "    print(np.average(res[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5360501567398119"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(index_list)\n",
    "train_set, test_set = set(index_list[:int(train_ratio*len(raw_training))]), set(index_list[int(train_ratio*len(raw_training)):])\n",
    "train_x, train_y, valid_x, valid_y = generate_training_set(raw_training, sid, train_set, test_set, last_days)\n",
    "sum([1 for i in range(len(valid_y)) if valid_y[i] > 0 ])/len(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPC(activation = 'relu', solver = 'adam', alpha = 0.1, hidden_layer_sizes = tuple((200,)*3), tol = 0.0001,max_iter= 200000, verbose =False, learning_rate = 'adaptive' )\n",
    "mlp.fit(train_x, train_y)\n",
    "accuracy(mlp, valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X, new_y = pd.concat([train_x,valid_x]).reset_index(drop = True),  train_y + valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = generate_test_set(raw_test, sid, last_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 200, 200), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPC(activation = 'relu', solver = 'adam', alpha = 0.1, hidden_layer_sizes = tuple((200,)*3), tol = 0.0001,max_iter= 200000, verbose =False, learning_rate = 'adaptive' )\n",
    "mlp.fit(new_X, new_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = mlp.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data['stock_directionality'] = final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_data.to_csv(write_dic, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
